{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from codetf.models import load_model_pipeline\n",
    "from sacrebleu.metrics import BLEU\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/paul/projects/edu/master/mdl-ii/src/data/cache/json/default-acdd91729f392843/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.94it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"test\": str(root_dir / \"data\" / \"test.jsonl\"),\n",
    "}, cache_dir=root_dir / \"data\" / \"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "def inference(doc):\n",
    "    doc[\"ref\"] = [\" \".join(docstring) for docstring in doc[\"docstring_tokens\"]]\n",
    "    return doc    \n",
    "\n",
    "dataset = dataset.map(inference, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def sina_xml_to_url_list(xml_data):\\n    rawur...</td>\n",
       "      <td>str - &gt; list Convert XML to URL List . From Bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def dailymotion_download(url, output_dir='.', ...</td>\n",
       "      <td>Downloads Dailymotion videos by URL .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def sina_download(url, output_dir='.', merge=T...</td>\n",
       "      <td>Downloads Sina videos by URL .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def sprint(text, *colors):\\n    return \"\\33[{}...</td>\n",
       "      <td>Format text with color or other effects into A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def print_log(text, *colors):\\n    sys.stderr....</td>\n",
       "      <td>Print a log message to standard error .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def sina_xml_to_url_list(xml_data):\\n    rawur...   \n",
       "1  def dailymotion_download(url, output_dir='.', ...   \n",
       "2  def sina_download(url, output_dir='.', merge=T...   \n",
       "3  def sprint(text, *colors):\\n    return \"\\33[{}...   \n",
       "4  def print_log(text, *colors):\\n    sys.stderr....   \n",
       "\n",
       "                                                 ref  \n",
       "0  str - > list Convert XML to URL List . From Bi...  \n",
       "1              Downloads Dailymotion videos by URL .  \n",
       "2                     Downloads Sina videos by URL .  \n",
       "3  Format text with color or other effects into A...  \n",
       "4            Print a log message to standard error .  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(type=\"pandas\", columns=[\"ref\", \"code\"])\n",
    "df = dataset[\"test\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m----> 5\u001b[0m     model\u001b[39m.\u001b[39;49mpredict([row[\u001b[39m\"\u001b[39;49m\u001b[39mcode\u001b[39;49m\u001b[39m\"\u001b[39;49m]])\n",
      "File \u001b[0;32m~/projects/edu/master/CodeTF/codetf/models/seq2seq_models/__init__.py:85\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict\u001b[0;34m(self, sources)\u001b[0m\n\u001b[1;32m     83\u001b[0m input_for_net \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(source\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit())\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m source \u001b[39min\u001b[39;00m sources]\n\u001b[1;32m     84\u001b[0m \u001b[39m# if self.task in [\"sum\", \"translate\", \"nl2code\", \"refine\"]:\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_for_net)\n\u001b[1;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/projects/edu/master/CodeTF/codetf/models/seq2seq_models/__init__.py:73\u001b[0m, in \u001b[0;36mSeq2SeqModel.forward\u001b[0;34m(self, sources)\u001b[0m\n\u001b[1;32m     71\u001b[0m input_ids \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     72\u001b[0m attention_mask \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39mattention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 73\u001b[0m generated_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     74\u001b[0m                                     max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_prediction_length, \n\u001b[1;32m     75\u001b[0m                                     num_beams\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_size)\n\u001b[1;32m     77\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/.conda/envs/codetf/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/codetf/lib/python3.10/site-packages/transformers/generation/utils.py:1611\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1604\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1605\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1606\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1607\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1608\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1609\u001b[0m     )\n\u001b[1;32m   1610\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1611\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1612\u001b[0m         input_ids,\n\u001b[1;32m   1613\u001b[0m         beam_scorer,\n\u001b[1;32m   1614\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1615\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1616\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1617\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1618\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1619\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1620\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1621\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1622\u001b[0m     )\n\u001b[1;32m   1624\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1625\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/.conda/envs/codetf/lib/python3.10/site-packages/transformers/generation/utils.py:2990\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2987\u001b[0m \u001b[39m# increase cur_len\u001b[39;00m\n\u001b[1;32m   2988\u001b[0m cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 2990\u001b[0m \u001b[39mif\u001b[39;00m beam_scorer\u001b[39m.\u001b[39mis_done \u001b[39mor\u001b[39;00m stopping_criteria(input_ids, scores):\n\u001b[1;32m   2991\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m synced_gpus:\n\u001b[1;32m   2992\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/codetf/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py:111\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mStoppingCriteriaList\u001b[39;00m(\u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[39m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    112\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids: torch\u001b[39m.\u001b[39mLongTensor, scores: torch\u001b[39m.\u001b[39mFloatTensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    113\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39many\u001b[39m(criteria(input_ids, scores) \u001b[39mfor\u001b[39;00m criteria \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n\u001b[1;32m    115\u001b[0m     \u001b[39m@property\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mmax_length\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = load_model_pipeline(model_name=\"codet5\", task=\"sum_python\", model_type=\"base\")\n",
    "preds = []\n",
    "\n",
    "for i, row in df.head(1000).iterrows():\n",
    "    if i % 100 == 0: print(i)\n",
    "    preds.append(model.predict([row[\"code\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = [\"\"\"\n",
    "def hello():\n",
    "    print(\"Hello world\")\n",
    "\"\"\"]\n",
    "\n",
    "model = load_model_pipeline(model_name=\"codet5\", task=\"sum_python\", model_type=\"base\")\n",
    "sys = model.predict(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 31.95 50.0/33.3/25.0/25.0 (BP = 1.000 ratio = 1.333 hyp_len = 4 ref_len = 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "bleu = BLEU()\n",
    "bleu.corpus_score(sys, [[\"prints hello world\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.get_signature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = [\"def sina_xml_to_url_list(xml_data):\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName('durl'):\\n        url = node.getElementsByTagName('url')[0]\\n        rawurl.append(url.childNodes[0].data)\\n    return rawurl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codetf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
