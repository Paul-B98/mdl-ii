{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/.conda/envs/tf/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (None)/charset_normalizer (3.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_files\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: root_dir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtrain.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m: root_dir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtest.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m: root_dir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mvalidation.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m })\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/datasets/load.py:1773\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1768\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1769\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1770\u001b[0m )\n\u001b[1;32m   1772\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1773\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1774\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1775\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1776\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1777\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1778\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1779\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1780\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1781\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1782\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1783\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1784\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1785\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1786\u001b[0m )\n\u001b[1;32m   1788\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/datasets/load.py:1502\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1501\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1502\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1503\u001b[0m     path,\n\u001b[1;32m   1504\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1505\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1506\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1507\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1508\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/datasets/load.py:1137\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[39m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \n\u001b[1;32m   1129\u001b[0m \u001b[39m# Try packaged\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m   1131\u001b[0m     \u001b[39mreturn\u001b[39;00m PackagedDatasetModuleFactory(\n\u001b[1;32m   1132\u001b[0m         path,\n\u001b[1;32m   1133\u001b[0m         data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1134\u001b[0m         data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1135\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1136\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m-> 1137\u001b[0m     )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1138\u001b[0m \u001b[39m# Try locally\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[39melif\u001b[39;00m path\u001b[39m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/datasets/load.py:712\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m base_path \u001b[39m=\u001b[39m (\n\u001b[1;32m    707\u001b[0m     \u001b[39mstr\u001b[39m(Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir)\u001b[39m.\u001b[39mexpanduser()\u001b[39m.\u001b[39mresolve()) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(Path()\u001b[39m.\u001b[39mresolve())\n\u001b[1;32m    708\u001b[0m )\n\u001b[1;32m    709\u001b[0m patterns \u001b[39m=\u001b[39m (\n\u001b[1;32m    710\u001b[0m     sanitize_patterns(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m get_data_patterns_locally(base_path)\n\u001b[1;32m    711\u001b[0m )\n\u001b[0;32m--> 712\u001b[0m data_files \u001b[39m=\u001b[39m DataFilesDict\u001b[39m.\u001b[39;49mfrom_local_or_remote(\n\u001b[1;32m    713\u001b[0m     patterns,\n\u001b[1;32m    714\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[1;32m    715\u001b[0m     base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[1;32m    716\u001b[0m )\n\u001b[1;32m    717\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m _MODULE_SUPPORTS_METADATA \u001b[39mand\u001b[39;00m patterns \u001b[39m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n\u001b[1;32m    718\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/datasets/data_files.py:783\u001b[0m, in \u001b[0;36mDataFilesDict.from_local_or_remote\u001b[0;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[1;32m    780\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m()\n\u001b[1;32m    781\u001b[0m \u001b[39mfor\u001b[39;00m key, patterns_for_key \u001b[39min\u001b[39;00m patterns\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    782\u001b[0m     out[key] \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 783\u001b[0m         DataFilesList\u001b[39m.\u001b[39;49mfrom_local_or_remote(\n\u001b[1;32m    784\u001b[0m             patterns_for_key,\n\u001b[1;32m    785\u001b[0m             base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[1;32m    786\u001b[0m             allowed_extensions\u001b[39m=\u001b[39;49mallowed_extensions,\n\u001b[1;32m    787\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    788\u001b[0m         )\n\u001b[1;32m    789\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[1;32m    790\u001b[0m         \u001b[39melse\u001b[39;00m patterns_for_key\n\u001b[1;32m    791\u001b[0m     )\n\u001b[1;32m    792\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/datasets/data_files.py:751\u001b[0m, in \u001b[0;36mDataFilesList.from_local_or_remote\u001b[0;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_local_or_remote\u001b[39m(\n\u001b[1;32m    744\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    748\u001b[0m     use_auth_token: Optional[Union[\u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    749\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFilesList\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    750\u001b[0m     base_path \u001b[39m=\u001b[39m base_path \u001b[39mif\u001b[39;00m base_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(Path()\u001b[39m.\u001b[39mresolve())\n\u001b[0;32m--> 751\u001b[0m     data_files \u001b[39m=\u001b[39m resolve_patterns_locally_or_by_urls(base_path, patterns, allowed_extensions)\n\u001b[1;32m    752\u001b[0m     origin_metadata \u001b[39m=\u001b[39m _get_origin_metadata_locally_or_by_urls(data_files, use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[1;32m    753\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(data_files, origin_metadata)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/datasets/data_files.py:346\u001b[0m, in \u001b[0;36mresolve_patterns_locally_or_by_urls\u001b[0;34m(base_path, patterns, allowed_extensions)\u001b[0m\n\u001b[1;32m    344\u001b[0m data_files \u001b[39m=\u001b[39m []\n\u001b[1;32m    345\u001b[0m \u001b[39mfor\u001b[39;00m pattern \u001b[39min\u001b[39;00m patterns:\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mif\u001b[39;00m is_remote_url(pattern):\n\u001b[1;32m    347\u001b[0m         data_files\u001b[39m.\u001b[39mappend(Url(pattern))\n\u001b[1;32m    348\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/site-packages/datasets/utils/file_utils.py:65\u001b[0m, in \u001b[0;36mis_remote_url\u001b[0;34m(url_or_filename)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_remote_url\u001b[39m(url_or_filename: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     parsed \u001b[39m=\u001b[39m urlparse(url_or_filename)\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed\u001b[39m.\u001b[39mscheme \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ms3\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhdfs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mftp\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/urllib/parse.py:391\u001b[0m, in \u001b[0;36murlparse\u001b[0;34m(url, scheme, allow_fragments)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39murlparse\u001b[39m(url, scheme\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, allow_fragments\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    372\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse a URL into 6 components:\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m    Note that % escapes are not expanded.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m     url, scheme, _coerce_result \u001b[39m=\u001b[39m _coerce_args(url, scheme)\n\u001b[1;32m    392\u001b[0m     splitresult \u001b[39m=\u001b[39m urlsplit(url, scheme, allow_fragments)\n\u001b[1;32m    393\u001b[0m     scheme, netloc, url, query, fragment \u001b[39m=\u001b[39m splitresult\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/urllib/parse.py:128\u001b[0m, in \u001b[0;36m_coerce_args\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m str_input:\n\u001b[1;32m    127\u001b[0m     \u001b[39mreturn\u001b[39;00m args \u001b[39m+\u001b[39m (_noop,)\n\u001b[0;32m--> 128\u001b[0m \u001b[39mreturn\u001b[39;00m _decode_args(args) \u001b[39m+\u001b[39m (_encode_result,)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/urllib/parse.py:112\u001b[0m, in \u001b[0;36m_decode_args\u001b[0;34m(args, encoding, errors)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode_args\u001b[39m(args, encoding\u001b[39m=\u001b[39m_implicit_encoding,\n\u001b[1;32m    111\u001b[0m                        errors\u001b[39m=\u001b[39m_implicit_errors):\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(x\u001b[39m.\u001b[39;49mdecode(encoding, errors) \u001b[39mif\u001b[39;49;00m x \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m args)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.10/urllib/parse.py:112\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode_args\u001b[39m(args, encoding\u001b[39m=\u001b[39m_implicit_encoding,\n\u001b[1;32m    111\u001b[0m                        errors\u001b[39m=\u001b[39m_implicit_errors):\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(x\u001b[39m.\u001b[39;49mdecode(encoding, errors) \u001b[39mif\u001b[39;00m x \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m args)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": str(root_dir / \"data\" / \"train.jsonl\"),\n",
    "    \"test\": str(root_dir / \"data\" / \"test.jsonl\"),\n",
    "    \"validation\": str(root_dir / \"data\" / \"validation.jsonl\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>original_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...</td>\n",
       "      <td>['def' 'split_phylogeny' '(' 'p' ',' 'level' '...</td>\n",
       "      <td>Return either the full or truncated version of...</td>\n",
       "      <td>['Return' 'either' 'the' 'full' 'or' 'truncate...</td>\n",
       "      <td>def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>def ensure_dir(d):\\n    \"\"\"\\n    Check to make...</td>\n",
       "      <td>['def' 'ensure_dir' '(' 'd' ')' ':' 'if' 'not'...</td>\n",
       "      <td>Check to make sure the supplied directory path...</td>\n",
       "      <td>['Check' 'to' 'make' 'sure' 'the' 'supplied' '...</td>\n",
       "      <td>def ensure_dir(d):\\n    \"\"\"\\n    Check to make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...</td>\n",
       "      <td>['def' 'file_handle' '(' 'fnh' ',' 'mode' '=' ...</td>\n",
       "      <td>Takes either a file path or an open file handl...</td>\n",
       "      <td>['Takes' 'either' 'a' 'file' 'path' 'or' 'an' ...</td>\n",
       "      <td>def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>def gather_categories(imap, header, categories...</td>\n",
       "      <td>['def' 'gather_categories' '(' 'imap' ',' 'hea...</td>\n",
       "      <td>Find the user specified categories in the map ...</td>\n",
       "      <td>['Find' 'the' 'user' 'specified' 'categories' ...</td>\n",
       "      <td>def gather_categories(imap, header, categories...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...</td>\n",
       "      <td>['def' 'parse_unifrac' '(' 'unifracFN' ')' ':'...</td>\n",
       "      <td>Parses the unifrac results file into a diction...</td>\n",
       "      <td>['Parses' 'the' 'unifrac' 'results' 'file' 'in...</td>\n",
       "      <td>def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               code  \\\n",
       "0           0  def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...   \n",
       "1           1  def ensure_dir(d):\\n    \"\"\"\\n    Check to make...   \n",
       "2           2  def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...   \n",
       "3           3  def gather_categories(imap, header, categories...   \n",
       "4           4  def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...   \n",
       "\n",
       "                                         code_tokens  \\\n",
       "0  ['def' 'split_phylogeny' '(' 'p' ',' 'level' '...   \n",
       "1  ['def' 'ensure_dir' '(' 'd' ')' ':' 'if' 'not'...   \n",
       "2  ['def' 'file_handle' '(' 'fnh' ',' 'mode' '=' ...   \n",
       "3  ['def' 'gather_categories' '(' 'imap' ',' 'hea...   \n",
       "4  ['def' 'parse_unifrac' '(' 'unifracFN' ')' ':'...   \n",
       "\n",
       "                                           docstring  \\\n",
       "0  Return either the full or truncated version of...   \n",
       "1  Check to make sure the supplied directory path...   \n",
       "2  Takes either a file path or an open file handl...   \n",
       "3  Find the user specified categories in the map ...   \n",
       "4  Parses the unifrac results file into a diction...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0  ['Return' 'either' 'the' 'full' 'or' 'truncate...   \n",
       "1  ['Check' 'to' 'make' 'sure' 'the' 'supplied' '...   \n",
       "2  ['Takes' 'either' 'a' 'file' 'path' 'or' 'an' ...   \n",
       "3  ['Find' 'the' 'user' 'specified' 'categories' ...   \n",
       "4  ['Parses' 'the' 'unifrac' 'results' 'file' 'in...   \n",
       "\n",
       "                                     original_string  \n",
       "0  def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...  \n",
       "1  def ensure_dir(d):\\n    \"\"\"\\n    Check to make...  \n",
       "2  def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...  \n",
       "3  def gather_categories(imap, header, categories...  \n",
       "4  def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(type=\"pandas\")\n",
    "train_df = dataset[\"train\"][:]\n",
    "test_df = dataset[\"test\"][:]\n",
    "validation_df = dataset[\"validation\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'def\\' \\'split_phylogeny\\' \\'(\\' \\'p\\' \\',\\' \\'level\\' \\'=\\' \\'\"s\"\\' \\')\\' \\':\\' \\'level\\' \\'=\\'\\n \\'level\\' \\'+\\' \\'\"__\"\\' \\'result\\' \\'=\\' \\'p\\' \\'.\\' \\'split\\' \\'(\\' \\'level\\' \\')\\' \\'return\\'\\n \\'result\\' \\'[\\' \\'0\\' \\']\\' \\'+\\' \\'level\\' \\'+\\' \\'result\\' \\'[\\' \\'1\\' \\']\\' \\'.\\' \\'split\\' \\'(\\'\\n \\'\";\"\\' \\')\\' \\'[\\' \\'0\\' \\']\\']'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[[0]][\"code_tokens\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
