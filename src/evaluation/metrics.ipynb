{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from codetf.models import load_model_pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from sacrebleu import corpus_bleu, corpus_chrf, corpus_ter\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/codetf/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1688104171460
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = Path.cwd()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1688104171704
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Create Predictions\n",
        "Im folgenden wird mit allen Models für den Testdatensatz die Predictions generiert."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractModel(ABC):\n",
        "    def predict(self, code: str) -> str:\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def model_name(self) -> str:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class CodeTFModel(AbstractModel):\n",
        "    def __init__(self, model_name: str, model_type: str, task: str) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._model = load_model_pipeline(model_name=model_name, model_type=model_type, task=task)\n",
        "        self._model_name = model_name\n",
        "        self._model_type = model_type\n",
        "        self._task = task\n",
        "\n",
        "    def predict(self, code: str) -> str:\n",
        "        return self._model.predict([code])[0]\n",
        "    \n",
        "    def model_name(self) -> str:\n",
        "        return f\"{self._model_name}-{self._model_type}-{self._task}\"\n",
        "\n",
        "\n",
        "class SebisModel(AbstractModel):\n",
        "    def __init__(self, model_name: str) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._pipeline = SummarizationPipeline(\n",
        "            model=AutoModelWithLMHead.from_pretrained(model_name),\n",
        "            tokenizer=AutoTokenizer.from_pretrained(model_name, skip_special_tokens=True),\n",
        "            device=0\n",
        "        )\n",
        "        self._model_name = model_name\n",
        "\n",
        "    def predict(self, code: str) -> str:\n",
        "        return self._pipeline([code])[0][\"summary_text\"]\n",
        "    \n",
        "    def model_name(self) -> str:\n",
        "        return self._model_name.replace(\"/\", \"-\")\n",
        "\n",
        "\n",
        "class CodeT5PModel(AbstractModel):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self._pipeline = SummarizationPipeline(\n",
        "            model=AutoModelWithLMHead.from_pretrained(root_dir.parent / \"modeling\" / \"models\" / \"codet5p_220m\"),\n",
        "            tokenizer=AutoTokenizer.from_pretrained(\"Salesforce/codet5p-220m\"),\n",
        "            device=0\n",
        "        )\n",
        "        self._model_name = \"codet5p_220m\"\n",
        "\n",
        "    def predict(self, code: str) -> str:\n",
        "        return self._pipeline([code])[0][\"summary_text\"]\n",
        "    \n",
        "    def model_name(self) -> str:\n",
        "        return self._model_name.replace(\"/\", \"-\")"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1688105503955
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preds(df: pd.DataFrame, model: AbstractModel):\n",
        "    file_path = root_dir / \"data\" / \"preds\" / f\"{model.model_name()}.csv\"\n",
        "    \n",
        "    if file_path.exists():\n",
        "        return\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"pred\"] = df[\"code\"].map(model.predict)\n",
        "    df[[\"ref\", \"pred\"]].to_csv(file_path)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1688104172036
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"json\", data_files={\n",
        "    \"test\": str(root_dir.parent / \"data\" / \"test.jsonl\"),\n",
        "}, cache_dir=root_dir.parent / \"data\" / \"cache\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found cached dataset json (/mnt/batch/tasks/shared/LS_root/mounts/clusters/mdl/code/Users/Paul.Brauckmann/mdl-ii/src/data/cache/json/default-55e4ec2f4ae22eff/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1688104172984
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(doc):\n",
        "    doc[\"ref\"] = [\" \".join(docstring) for docstring in doc[\"docstring_tokens\"]]\n",
        "    return doc    \n",
        "\n",
        "dataset = dataset.map(inference, batched=True)\n",
        "dataset.set_format(type=\"pandas\", columns=[\"ref\", \"code\"])\n",
        "df = dataset[\"test\"][:]\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Loading cached processed dataset at /mnt/batch/tasks/shared/LS_root/mounts/clusters/mdl/code/Users/Paul.Brauckmann/mdl-ii/src/data/cache/json/default-55e4ec2f4ae22eff/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-fe04db097f4167f6.arrow\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "                                                code  \\\n0  def sina_xml_to_url_list(xml_data):\\n    rawur...   \n1  def dailymotion_download(url, output_dir='.', ...   \n2  def sina_download(url, output_dir='.', merge=T...   \n3  def sprint(text, *colors):\\n    return \"\\33[{}...   \n4  def print_log(text, *colors):\\n    sys.stderr....   \n\n                                                 ref  \n0  str - > list Convert XML to URL List . From Bi...  \n1              Downloads Dailymotion videos by URL .  \n2                     Downloads Sina videos by URL .  \n3  Format text with color or other effects into A...  \n4            Print a log message to standard error .  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n      <th>ref</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>def sina_xml_to_url_list(xml_data):\\n    rawur...</td>\n      <td>str - &gt; list Convert XML to URL List . From Bi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>def dailymotion_download(url, output_dir='.', ...</td>\n      <td>Downloads Dailymotion videos by URL .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>def sina_download(url, output_dir='.', merge=T...</td>\n      <td>Downloads Sina videos by URL .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>def sprint(text, *colors):\\n    return \"\\33[{}...</td>\n      <td>Format text with color or other effects into A...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>def print_log(text, *colors):\\n    sys.stderr....</td>\n      <td>Print a log message to standard error .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1688104174013
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## CodeT5 Base"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, CodeTFModel(model_name=\"codet5\", model_type=\"base-multi-sum\", task=\"pretrained\"))"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1687842506988
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, CodeTFModel(model_name=\"codet5\", model_type=\"base\", task=\"sum_python\"))"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1687842508513
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## T5 Small"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_small_code_documentation_generation_python\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/codetf/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1400: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n/anaconda/envs/codetf/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1687844672640
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_small_code_documentation_generation_python_transfer_learning_finetune\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 641/641 [00:00<00:00, 3.57MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 242M/242M [00:02<00:00, 85.9MB/s]\nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 147kB/s]\nDownloading (…)ve/main/spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 90.4MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 11.6MB/s]\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1687846378088
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_small_code_documentation_generation_python_multitask\"))"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1687848066440
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_small_code_documentation_generation_python_multitask_finetune\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 641/641 [00:00<00:00, 4.10MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 242M/242M [00:03<00:00, 65.2MB/s] \nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 165kB/s]\nDownloading (…)ve/main/spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 86.6MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 13.1MB/s]\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1687849736449
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## T5 Base"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_base_code_documentation_generation_python\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 644/644 [00:00<00:00, 4.14MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 892M/892M [00:15<00:00, 57.5MB/s] \nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 160kB/s]\nDownloading (…)ve/main/spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 87.9MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 12.5MB/s]\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1687853459065
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_base_code_documentation_generation_python_transfer_learning_finetune\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 644/644 [00:00<00:00, 4.15MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 892M/892M [00:09<00:00, 96.5MB/s] \nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 162kB/s]\nDownloading (…)ve/main/spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 72.0MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 12.9MB/s]\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1687856583665
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_base_code_documentation_generation_python_multitask\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 644/644 [00:00<00:00, 3.33MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 892M/892M [00:10<00:00, 84.7MB/s] \nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 146kB/s]\nDownloading (…)ve/main/spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 76.9MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 12.5MB/s]\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1687859301142
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_base_code_documentation_generation_python_multitask_finetune\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 644/644 [00:00<00:00, 4.19MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 892M/892M [00:11<00:00, 81.0MB/s] \nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 165kB/s]\nDownloading (…)ve/main/spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 83.1MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 12.1MB/s]\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1687863268607
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## T5 Large"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_large_code_documentation_generation_python_transfer_learning_finetune\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 645/645 [00:00<00:00, 3.11MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 2.95G/2.95G [00:42<00:00, 68.6MB/s]\nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 85.9kB/s]\nDownloading spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 155MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 10.2MB/s]\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1687869044748
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_large_code_documentation_generation_python_multitask\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 645/645 [00:00<00:00, 4.29MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 2.95G/2.95G [00:42<00:00, 69.5MB/s]\nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 137kB/s]\nDownloading spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 3.85MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 11.6MB/s]\nBad pipe message: %s [b'\\xb7\\x8d\\xd5\\x1ac\\xc4U\\xbem\\xc0\\x95', b'\\x0f\\x7fn\\xa3\\x0c W\\xf3vwf\\xd12\\xd0\\x89Rig\\x04\\xc8M\\xe7\\x8e\\xa7B\\x8d;>Q\\x15\\xca\\xd3>\\xc6\\x8f.Z\\xb5\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00']\nBad pipe message: %s [b'\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xf6\\x86\\xbb\\xc5\\x15I30;\\x0b+\\x8eZ\\x8br=\\xc7ft\\x00\\x98PFo\\x06\\xe4\\xe9\\xf7\\xfab;']\nBad pipe message: %s [b'\\xca\\x97\\xca\\xed\\xac\\x04\\xce<\\xc2n$\\xa1\\x97j\\xf6\\xd0\\x8e\\xa8\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3']\nBad pipe message: %s [b\"\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\", b'']\nBad pipe message: %s [b'', b'\\x03\\x03']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'\\x98\\xfd|f\\xea\\xbf\\x9b']\nBad pipe message: %s [b'\\xa3\\xd2']\nBad pipe message: %s [b'>g\\xf3\\x95\\x03k+\\xa2E\\xe9Z\\x05pU\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\nBad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\nBad pipe message: %s [b'\\xb2+\\xa6\\xd1\\xff5z\\xd4x\\xb1b>\\x89\\x1c`']\nBad pipe message: %s [b'p\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00']\nBad pipe message: %s [b'\\xff\\x02\\x01']\nBad pipe message: %s [b'\\xdfF\\x9c\\x8e,\\n\\xa8\\xc9S\\x8e\\x08t\\x02\\xba&w\\xdb\\x92\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00']\nBad pipe message: %s [b'\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c']\nBad pipe message: %s [b\"\\x1e\\x7f\\x15\\x96\\x8e\\xa4Y\\x16o\\x9e!\\x1f\\xe7\\xb9\\xc9'\\xc0\\xbb\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\"]\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1687930141929
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, SebisModel(\"SEBIS/code_trans_t5_large_code_documentation_generation_python_multitask_finetune\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading (…)lve/main/config.json: 100%|██████████| 645/645 [00:00<00:00, 4.16MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 2.95G/2.95G [00:52<00:00, 56.6MB/s]\nDownloading (…)okenizer_config.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 135kB/s]\nDownloading spiece.model: 100%|██████████| 797k/797k [00:00<00:00, 3.84MB/s]\nDownloading (…)cial_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 3.73MB/s]\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1687930142460
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuned Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_preds(df, CodeT5PModel())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/codetf/lib/python3.10/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\nToken indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688108984542
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Auswertung"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scores(df: pd.DataFrame, model_name: str):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
        "    scores = [scorer.score(ref, hyp) for ref, hyp in zip(df[\"ref\"].to_list(), df[\"pred\"].to_list())]\n",
        "    rouge1 = sum([score[\"rouge1\"].fmeasure for score in scores]) / len(scores)\n",
        "    rougeL = sum([score[\"rougeL\"].fmeasure for score in scores]) / len(scores)\n",
        "    # meteor = sum([meteor_score([ref], hyp) for ref, hyp in zip(df[\"ref\"].to_list(), df[\"pred\"].to_list())]) / len(df[\"pred\"].to_list())\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"bleu\": corpus_bleu(df[\"pred\"].to_list(), [df[\"ref\"].to_list()]).score,\n",
        "        \"chrf\": corpus_chrf(df[\"pred\"].to_list(), [df[\"ref\"].to_list()]).score,\n",
        "        \"ter\": corpus_ter(df[\"pred\"].to_list(), [df[\"ref\"].to_list()]).score,\n",
        "        \"rouge1\": rouge1, \"rougeL\": rougeL\n",
        "    }, index=pd.Index([model_name], name=\"Model\"))"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1688109149187
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=[\"bleu\", \"chrf\", \"ter\"])\n",
        "\n",
        "for csv_file in sorted((root_dir / \"data\" / \"preds\").glob(\"*.csv\"), key=lambda f: f.name):\n",
        "    df = pd.concat([df, get_scores(pd.read_csv(csv_file, index_col=0), csv_file.name)])\n",
        "\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\nWARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\nWARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\nWARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "                                                         bleu       chrf  \\\nSEBIS-code_trans_t5_base_code_documentation_gen...   4.637784  23.193613   \nSEBIS-code_trans_t5_base_code_documentation_gen...   2.956659  15.565684   \nSEBIS-code_trans_t5_base_code_documentation_gen...  13.765888  33.452465   \nSEBIS-code_trans_t5_base_code_documentation_gen...  21.670814  37.954286   \nSEBIS-code_trans_t5_large_code_documentation_ge...  13.487266  32.526960   \nSEBIS-code_trans_t5_large_code_documentation_ge...  16.361668  35.033486   \nSEBIS-code_trans_t5_large_code_documentation_ge...  23.306288  38.984149   \nSEBIS-code_trans_t5_small_code_documentation_ge...   5.495179  25.007326   \nSEBIS-code_trans_t5_small_code_documentation_ge...   5.449546  20.280297   \nSEBIS-code_trans_t5_small_code_documentation_ge...  16.378271  34.691570   \nSEBIS-code_trans_t5_small_code_documentation_ge...  21.092646  37.156618   \ncodet5-base-multi-sum-pretrained.csv                23.563699  39.068588   \ncodet5-base-sum_python.csv                          23.984672  39.317939   \ncodet5p_220m.csv                                    25.244713  40.596060   \n\n                                                           ter    rouge1  \\\nSEBIS-code_trans_t5_base_code_documentation_gen...  102.020691  0.263155   \nSEBIS-code_trans_t5_base_code_documentation_gen...   93.159572  0.220844   \nSEBIS-code_trans_t5_base_code_documentation_gen...   78.741550  0.442934   \nSEBIS-code_trans_t5_base_code_documentation_gen...   71.560100  0.485289   \nSEBIS-code_trans_t5_large_code_documentation_ge...   79.615003  0.433306   \nSEBIS-code_trans_t5_large_code_documentation_ge...   80.671383  0.444580   \nSEBIS-code_trans_t5_large_code_documentation_ge...   69.744748  0.497358   \nSEBIS-code_trans_t5_small_code_documentation_ge...  103.980811  0.286502   \nSEBIS-code_trans_t5_small_code_documentation_ge...   89.909868  0.295190   \nSEBIS-code_trans_t5_small_code_documentation_ge...   76.738425  0.452363   \nSEBIS-code_trans_t5_small_code_documentation_ge...   71.625518  0.476070   \ncodet5-base-multi-sum-pretrained.csv                 93.312820  0.489486   \ncodet5-base-sum_python.csv                           88.666925  0.491368   \ncodet5p_220m.csv                                     66.514428  0.515171   \n\n                                                      rougeL  \nSEBIS-code_trans_t5_base_code_documentation_gen...  0.233785  \nSEBIS-code_trans_t5_base_code_documentation_gen...  0.208371  \nSEBIS-code_trans_t5_base_code_documentation_gen...  0.410086  \nSEBIS-code_trans_t5_base_code_documentation_gen...  0.457306  \nSEBIS-code_trans_t5_large_code_documentation_ge...  0.401295  \nSEBIS-code_trans_t5_large_code_documentation_ge...  0.412247  \nSEBIS-code_trans_t5_large_code_documentation_ge...  0.470244  \nSEBIS-code_trans_t5_small_code_documentation_ge...  0.255965  \nSEBIS-code_trans_t5_small_code_documentation_ge...  0.276226  \nSEBIS-code_trans_t5_small_code_documentation_ge...  0.421350  \nSEBIS-code_trans_t5_small_code_documentation_ge...  0.448324  \ncodet5-base-multi-sum-pretrained.csv                0.461698  \ncodet5-base-sum_python.csv                          0.463086  \ncodet5p_220m.csv                                    0.487640  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bleu</th>\n      <th>chrf</th>\n      <th>ter</th>\n      <th>rouge1</th>\n      <th>rougeL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>SEBIS-code_trans_t5_base_code_documentation_generation_python.csv</th>\n      <td>4.637784</td>\n      <td>23.193613</td>\n      <td>102.020691</td>\n      <td>0.263155</td>\n      <td>0.233785</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_base_code_documentation_generation_python_multitask.csv</th>\n      <td>2.956659</td>\n      <td>15.565684</td>\n      <td>93.159572</td>\n      <td>0.220844</td>\n      <td>0.208371</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_base_code_documentation_generation_python_multitask_finetune.csv</th>\n      <td>13.765888</td>\n      <td>33.452465</td>\n      <td>78.741550</td>\n      <td>0.442934</td>\n      <td>0.410086</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_base_code_documentation_generation_python_transfer_learning_finetune.csv</th>\n      <td>21.670814</td>\n      <td>37.954286</td>\n      <td>71.560100</td>\n      <td>0.485289</td>\n      <td>0.457306</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_large_code_documentation_generation_python_multitask.csv</th>\n      <td>13.487266</td>\n      <td>32.526960</td>\n      <td>79.615003</td>\n      <td>0.433306</td>\n      <td>0.401295</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_large_code_documentation_generation_python_multitask_finetune.csv</th>\n      <td>16.361668</td>\n      <td>35.033486</td>\n      <td>80.671383</td>\n      <td>0.444580</td>\n      <td>0.412247</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_large_code_documentation_generation_python_transfer_learning_finetune.csv</th>\n      <td>23.306288</td>\n      <td>38.984149</td>\n      <td>69.744748</td>\n      <td>0.497358</td>\n      <td>0.470244</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_small_code_documentation_generation_python.csv</th>\n      <td>5.495179</td>\n      <td>25.007326</td>\n      <td>103.980811</td>\n      <td>0.286502</td>\n      <td>0.255965</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_small_code_documentation_generation_python_multitask.csv</th>\n      <td>5.449546</td>\n      <td>20.280297</td>\n      <td>89.909868</td>\n      <td>0.295190</td>\n      <td>0.276226</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_small_code_documentation_generation_python_multitask_finetune.csv</th>\n      <td>16.378271</td>\n      <td>34.691570</td>\n      <td>76.738425</td>\n      <td>0.452363</td>\n      <td>0.421350</td>\n    </tr>\n    <tr>\n      <th>SEBIS-code_trans_t5_small_code_documentation_generation_python_transfer_learning_finetune.csv</th>\n      <td>21.092646</td>\n      <td>37.156618</td>\n      <td>71.625518</td>\n      <td>0.476070</td>\n      <td>0.448324</td>\n    </tr>\n    <tr>\n      <th>codet5-base-multi-sum-pretrained.csv</th>\n      <td>23.563699</td>\n      <td>39.068588</td>\n      <td>93.312820</td>\n      <td>0.489486</td>\n      <td>0.461698</td>\n    </tr>\n    <tr>\n      <th>codet5-base-sum_python.csv</th>\n      <td>23.984672</td>\n      <td>39.317939</td>\n      <td>88.666925</td>\n      <td>0.491368</td>\n      <td>0.463086</td>\n    </tr>\n    <tr>\n      <th>codet5p_220m.csv</th>\n      <td>25.244713</td>\n      <td>40.596060</td>\n      <td>66.514428</td>\n      <td>0.515171</td>\n      <td>0.487640</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1688113845413
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "codetf",
      "language": "python",
      "display_name": "codetf"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "codetf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}